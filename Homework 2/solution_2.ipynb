{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1- small data learning with embeddings (30%)\n",
    "### We're going to use pre-trained embeddings to try to learn a text classification problem with few training examples\n",
    "### This is very similar to what we did in class!\n",
    "## $ \\\\ $\n",
    "## Part 0: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#%pylab inline\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=1234)\n",
    "data_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'), random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:\n",
    "### a. What is the most common class in the train set?\n",
    "### b. What is the out of sample (test) accuracy if we guess the most probable class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint: you can do this in many ways, including collections.Counter or pandas\n",
    "print('most common class {}: {}'.format(most_common_class, data_train.target_names[most_common_class]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Turn the text into integer sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 10000\n",
    "MAX_SEQ_LEN = 100\n",
    "EMBEDDING_DIM = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Instantiate a tokenizer with max workds\n",
    "# 2. fit the tokenizer on text\n",
    "# 3. Turn the text into integer sequences (train and test)\n",
    "# 4. pad the sequences to a constant sequence length (train and test)\n",
    "# 5. turn y into categorical variables\n",
    "\n",
    "\n",
    "# you should have 4 variables:\n",
    "# y_train, y_test, int_sequences_train, int_sequences_test\n",
    "# all are numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert y_train.shape ==(11314, 20), 'something went wrong'\n",
    "assert int_sequences_test.shape == (7532, 100), 'something went wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: load the GloVe embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = ''  # FIXME directory with glove\n",
    "GLOVE_PATH = os.path.join(GLOVE_DIR, 'glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_file(filepath):\n",
    "    word_to_vector = {}\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_to_vector[word] = vector\n",
    "    return word_to_vector\n",
    "\n",
    "word_vecs = load_glove_file(GLOVE_PATH)\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for word, i in tok.word_index.items():\n",
    "    if i >= MAX_WORDS:\n",
    "        continue\n",
    "    embedding_vector = word_vecs.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_CLASSES = y_train.shape[1]\n",
    "assert NUM_CLASSES == 20, 'something went wrong'\n",
    "NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dropout, Dense, GlobalAveragePooling1D\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "# TODO\n",
    "# 1. Build a model with\n",
    "#  - an embedding\n",
    "#  - some number of dense layers\n",
    "#  - dropout\n",
    "#  - don't forget to use GlobalAveragePooling to average over one dimension\n",
    "\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "word_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "\n",
    "# Add code here\n",
    "\n",
    "# output = ...\n",
    "\n",
    "model = Model(word_input, output)\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "516084"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_to_train = 100\n",
    "epochs = 1000  # this is a big number but won't take long with 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2529208709506107"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(\n",
    "    int_sequences_train[:num_samples_to_train], \n",
    "    y_train[:num_samples_to_train], \n",
    "    epochs=1000, shuffle=True, batch_size=num_samples_to_train, verbose=0\n",
    ")\n",
    "accuracy_score(np.argmax(y_test, axis=1), np.argmax(model.predict(int_sequences_test), axis=1).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## you should be able to get more than 20% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Compare to others methods\n",
    "### a. How does this compare to a randomly initialized, trainable embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Build the same model as above, but with a random embedding\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "word_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "\n",
    "# your code here\n",
    "\n",
    "# output = ...\n",
    "\n",
    "model = Model(word_input, output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10050451407328731"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    int_sequences_train[:num_samples_to_train], \n",
    "    y_train[:num_samples_to_train], \n",
    "    epochs=1000, shuffle=True, batch_size=num_samples_to_train, verbose=0\n",
    ")\n",
    "accuracy_score(np.argmax(y_test, axis=1), np.argmax(model.predict(int_sequences_test), axis=1).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b: how does this compare to logistic regression trained on 100 samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12413701540095592"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "data_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# TODO\n",
    "# 1. make a count vectorizer\n",
    "# 2. fit it on only `samples_to_train` data points\n",
    "# 3. trainsform train and test data into integers\n",
    "# 4. fit logistic regression on just `num_samples_to_train` samples\n",
    "# 5. Compute accuracy score\n",
    "\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# your code here\n",
    "\n",
    "accuracy_score(data_test.target, lr.predict(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This should be approximately 10-12%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Homework problem: improving BOW (30%)\n",
    "There are many improvements that can be made to the bag of words represetation, without resorting to neural networks.\n",
    "Here we'll try one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe to restart notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: fit a bag of words and logistic regression to the 20 newsgroups data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "data_test = fetch_20newsgroups(subset='test',remove=('headers', 'footers', 'quotes'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo\n",
    "# 1. make a count vectorizer with max_features=20000\n",
    "# 2. fit it\n",
    "# 3. transform the train and test data into number\n",
    "vec = CountVectorizer(max_features=20000)\n",
    "vec.fit(data_train.data)\n",
    "xtr = vec.transform(data_train.data)\n",
    "xte = vec.transform(data_test.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6064790228359002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "# 1. fit logistic regression\n",
    "# 2. compute accuracy score\n",
    "lr = LogisticRegression()\n",
    "lr.fit(xtr, data_train.target)   # to be removed\n",
    "accuracy_score(data_test.target, lr.predict(xte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: TFIDF\n",
    "A big problem with counting words is that we'll tend to overweight very common words. \n",
    "These common words often carry little information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 93969),\n",
       " ('to', 51191),\n",
       " ('of', 45608),\n",
       " ('a', 40042),\n",
       " ('and', 39197),\n",
       " ('is', 28204),\n",
       " ('in', 27756),\n",
       " ('I', 27143),\n",
       " ('that', 25016),\n",
       " ('for', 18066)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def word_iterator():\n",
    "    \"\"\"This iterator yields one word at a time from the train data\"\"\"\n",
    "    for doc in data_train.data:\n",
    "        for word in doc.split():\n",
    "            yield word\n",
    "\n",
    "Counter(word_iterator()).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF is a scheme that combats this. \n",
    "## TFIDF = $\\text{Term Frequency Inverse Document Frequency} $\n",
    "# $ \\\\ $\n",
    "\n",
    "# $TFIDF\\left(d, t\\right) \\equiv \\frac{\\text{ Count}\\left(d, t\\right)}{\\text{Doc-Freq}\\left(d, t\\right)} \\equiv \\text{ Count}\\left(d, t\\right)\\,\\left(1 + log\\left( \\frac{N_{docs}}{df_{t}}\\right)\\right)$\n",
    "## Where\n",
    "### $df_{t}$ is the number of documents in which term $t$ appears\n",
    "### $N_{docs}$ is the total number of documents\n",
    "### $\\text{Count}\\left(d,t\\right)$ is the number of times term $t$ appears in document $d$ (the count matrix)\n",
    "\n",
    "# $ \\\\ $ \n",
    "# $ \\\\ $ \n",
    "## Like this, we suppress the weight of common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a: write turn the count matrix into a TFIDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_idf_vector(count_matrix):\n",
    "    \"\"\"Get the inverse document frequence vector (shape = num_words)\"\"\"\n",
    "    df = np.array((count_matrix > 0).astype(int).sum(axis=0))\n",
    "    return np.log(count_matrix.shape[0] / (1+df))\n",
    "\n",
    "\n",
    "#TODO(fill in this function)\n",
    "def get_tfidf_matrix(count_matrix):\n",
    "    \"\"\"Turn a count matrix into a tfidf matrix\"\"\"\n",
    "    # TODO\n",
    "#     1. get the idfs with the aboce function\n",
    "#     2. turn it into a numpy array `with .toarray()`\n",
    "#     3. loop the the ROWS of the matrix and transform them\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr_transformed = get_tfidf_matrix(xtr)\n",
    "xte_transformed = get_tfidf_matrix(xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It worked!\n"
     ]
    }
   ],
   "source": [
    "assert xtr_transformed.shape == xtr.shape, 'something has gone wrong'\n",
    "print('It worked!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6251991502920871"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(xtr_transformed, data_train.target)\n",
    "accuracy_score(data_test.target, lr.predict(xte_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Do the same with scikitlearn's implmenetation\n",
    "### Happily, sklearn does this for us\n",
    "### And it includes some other nice normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6715347849176846"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TODO:\n",
    "# 1. instantiate a TfidfVectorizer with max_features = 20000\n",
    "# 2. fit it on the train data\n",
    "# 3. transform train and test data into matrices\n",
    "# 4. fit logistic regression on the train data\n",
    "# 5. compute the accuracy score on the test data\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(max_features=20000)\n",
    "\n",
    "# Your code here\n",
    "\n",
    "accuracy_score(data_test.target, lr.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Tuning the number of words to use\n",
    "## Make a plot of how the vocabulary size (`max_features`) impacts results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this took 59.59 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "results = {}\n",
    "for max_features in (100, 500, 1000, 5000, 10000, 20000, 50000, None):\n",
    "    # TODO:\n",
    "    # 1. instantiate a TfidfVectorizer with max_features = max_features\n",
    "    # 2. fit it on the train data\n",
    "    # 3. transform train and test data into matrices\n",
    "    # 4. fit logistic regression on the train data\n",
    "    # 5. compute the accuracy score on the test data\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    # x_train = ...\n",
    "    # x_test = ...\n",
    "\n",
    "    lr = LogisticRegression()\n",
    "    # lr.fit(...\n",
    "    \n",
    "    if max_features is None:\n",
    "        num_features = len(vec.get_feature_names())\n",
    "    else:\n",
    "        num_features = max_features\n",
    "    results[num_features] = accuracy_score(data_test.target, lr.predict(x_test)) # to be removed\n",
    "\n",
    "print('this took {:.2f} seconds'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(results).plot(figsize=(12,8), fontsize=16)\n",
    "plt.xlabel('max features', fontsize=16)\n",
    "plt.ylabel('out of sample accuracy', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Named entity recognition (40 %)\n",
    "\n",
    "Named entity recognition is a common NLP task that tries to identify entities in text.\n",
    "\n",
    "See: https://en.wikipedia.org/wiki/Named-entity_recognition\n",
    "\n",
    "Common Types of entities include `Locations`, `People`, and `Organizations`. For example, in the sentence\n",
    "# Janet Yellen, the chairwoman of the Federal Reserve, gave a speech in Colorado.\n",
    "## $ \\\\ $ \n",
    "the goal would be to recognize\n",
    "# `Janet Yellen`$_{PERSON}$, the chairwoman of the `Federal Reserve`$_{ORGANIZATION}$, gave a speech in `Colorado`$_{LOCATION}$.\n",
    "# $ \\\\ $\n",
    "# In this problem we will build a model to recognized named entities using word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: \n",
    "### Give an example of a sentence with a Person but not a location. \n",
    "### Give an example of a sentence with an organization and a location, but not a person. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put it here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: building a model\n",
    "# $ \\\\ $\n",
    "### The goal of this section is to build a model to take a sentence (list of words) and identify what kind of tag each word is\n",
    "# $ \\\\ $\n",
    "## Why is this problem hard:\n",
    "### Some words will be the same tag all the time. For example `Colorado` is almost always a `LOCATION`\n",
    "### Some words depend on context: above `federal` and `reserve` are `ORGANIZATION` but I can write `I would like to reserve a table.`\n",
    "\n",
    "# $ \\\\ $\n",
    "## To combat this issue we will make a very simple model but taking a 3-word window around every word\n",
    "  - For every word, we will take the word vector of that word and the two surrounding words\n",
    "\n",
    "### Example: `I went to the store` will be represented as \n",
    " - `I` $\\rightarrow$ `UKNOWN` - `I` - `went` $\\rightarrow$ $\\left[ V_{UKNOWN}, V_{I}, V_{went}\\right]$\n",
    " - `went` $\\rightarrow$ `I` - `went` - `to`$\\rightarrow$ $\\left[V_{I}, V_{went},  V_{to}\\right]$\n",
    " - `to` $\\rightarrow$ `went` - `to` - `the`$\\rightarrow$ $\\left[V_{went},  V_{to}, V_{the}\\right]$\n",
    " - ...\n",
    "#### Where \n",
    " - $V_{word_{i}}$ is the representation for $word_{i}$\n",
    " - `UKNOWN` is the token for unknown or boundary words\n",
    " \n",
    "Like this, we will encode some __context__ around every word. Each word here will be encoded as a $3 * d_{embedding}$-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "from keras.initializers import Constant\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_file(filepath):\n",
    "    \"\"\"Load a glove embedding from a file\"\"\"\n",
    "    word_to_vector = {}\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word_to_vector[word] = vector\n",
    "    return word_to_vector\n",
    "\n",
    "def load_dataset(fname):\n",
    "    \"\"\"Load an NER dataset\"\"\"\n",
    "    docs = []\n",
    "    with open(fname) as fd:\n",
    "        cur = []\n",
    "        for line in fd:\n",
    "            line = line.lower()\n",
    "            # new sentence on -DOCSTART- or blank line\n",
    "            if re.match(r\"-DOCSTART-.+\".lower(), line) or (len(line.strip()) == 0):\n",
    "                if len(cur) > 0:\n",
    "                    docs.append(cur)\n",
    "                cur = []\n",
    "            else: # read in tokens\n",
    "                cur.append(line.strip().split(\"\\t\",1))\n",
    "        # flush running buffer\n",
    "        if cur:\n",
    "            docs.append(cur)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GLOVE_DIR = ''  # FIXME directory with glove\n",
    "DATA_PATH = ''  # where you downloaded the data\n",
    "\n",
    "word_vecs = load_glove_file(os.path.join(GLOVEIDR, 'glove.6B.50d.txt'))\n",
    "docs = load_dataset(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_first_doc = [\n",
    "    ['eu', 'org'],\n",
    "    ['rejects', 'o'],\n",
    "    ['german', 'misc'],\n",
    "    ['call', 'o'],\n",
    "    ['to', 'o'],\n",
    "    ['boycott', 'o'],\n",
    "    ['british', 'misc'],\n",
    "    ['lamb', 'o'],\n",
    "    ['.', 'o'],\n",
    "]\n",
    "assert len(word_vecs) == 400000, 'word vectors did not load properly'\n",
    "assert word_vecs['the'].shape == (50,), 'word vectors did not load properly'\n",
    "assert len(docs) == 14041, 'something has gone wrong with data loading'\n",
    "assert docs[0] == correct_first_doc, 'something has gone wrong with data loading'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = len(word_vecs)  # max number of words to use in the embedding\n",
    "UNKNOWN = 'UUUNKKK'.lower()  # token for unknown word\n",
    "UNKNOWN_WORD_INDEX = 0\n",
    "EMBEDDING_DIM = 50  # dimension of embedding\n",
    "NULL_TAG = 'o'  # tags that are not a named entity\n",
    "\n",
    "# Some derived quantities\n",
    "TAGS = (NULL_TAG, 'loc', 'per', 'org', 'misc')\n",
    "NUM_TO_TAG = dict(enumerate(TAGS))\n",
    "TAG_TO_NUM = {tag: num for num, tag in NUM_TO_TAG.items()}\n",
    "\n",
    "NUM_CLASSES = len(TAGS)\n",
    "assert NUM_CLASSES == 5, 'somethig has gone wrong'\n",
    "\n",
    "WINDOW = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_num = {word: idx + 1 for idx, word in enumerate(word_vecs.keys())}\n",
    "num_to_word = {num: word for word, num in word_to_num.items()}\n",
    "\n",
    "word_to_num[UNKNOWN] = UNKNOWN_WORD_INDEX\n",
    "num_to_word[UNKNOWN_WORD_INDEX] = UNKNOWN\n",
    "\n",
    "assert word_to_num['the'] < 10, '\"the\" is not a common word- something has gone wrong.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix\n",
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "for word, i in word_to_num.items():#tok.word_index.items():\n",
    "    if i >= MAX_WORDS:\n",
    "        continue\n",
    "    embedding_vector = word_vecs.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating windowed-word sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_windows(words, tags, word_to_num, tag_to_num, left=WINDOW, right=WINDOW):\n",
    "    \"\"\"Turn sequences of words and tags into corresponding windowed sequences\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    word_dict = {ind: word for ind, word in enumerate(words)}\n",
    "    for i, word in enumerate(words):\n",
    "        if word == \"<s>\" or word == \"</s>\":\n",
    "            continue # skip sentence delimiters\n",
    "        word_seq = [word_dict.get(i + ii, UNKNOWN) for ii in range(-left, 1 + right)]\n",
    "        int_seq = [word_to_num.get(w, UNKNOWN_WORD_INDEX) for w in word_seq]\n",
    "        tagn = tag_to_num[tags[i]] \n",
    "        X.append(int_seq)\n",
    "        y.append(tagn)\n",
    "    return array(X), array(y)\n",
    "\n",
    "\n",
    "def window_row_to_vector(window_row, embed_matrix):\n",
    "    \"\"\"Turn a row of integers (np.array) into a single word vector\"\"\"\n",
    "    # TODO: implement this\n",
    "    return np.hstack([embed_matrix[i] for i in window_row]) # to be removed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = zip(*docs[0])\n",
    "x, y = seq_to_windows(words, tags, word_to_num=word_to_num, tag_to_num=TAG_TO_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x.dtype == np.int, 'x has the wrong data type'\n",
    "reconstructed_words = [num_to_word[num] for num in x[:, WINDOW]]\n",
    "assert tuple(reconstructed_words) == words, 'word transformation has gone wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xs = []\n",
    "all_ys = []\n",
    "for doc in docs:\n",
    "#     TODO\n",
    "#     1. unpack the words and the tags from `docs`\n",
    "#     2. use `seq_to_windows` to turn `words` and `tag` into `x` and `y`\n",
    "#     3. turn `x` into a single vector with `window_row_to_vector`\n",
    "    \n",
    "    words, tags = zip(*doc)\n",
    "    \n",
    "    # Your code here\n",
    "    \n",
    "    all_xs.extend(x)\n",
    "    all_ys.extend(y)\n",
    "    \n",
    "    \n",
    "all_xs = np.vstack(all_xs)\n",
    "all_ys = np.vstack(all_ys)\n",
    "\n",
    "\n",
    "all_ys = to_categorical(all_ys)\n",
    "assert all_xs.shape[0] == all_ys.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. make an array of indices to be shuffled with `np.arange`\n",
    "# 2. shuffle the indices randomly\n",
    "# 3. use the shuffled indices to shuffle `all_xs` and `all_ys`\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# inds = np.arange(....\n",
    "\n",
    "cut = int(0.8 * all_xs.shape[0])\n",
    "x_train, x_val = all_xs[:cut], all_xs[cut:]\n",
    "y_train, y_val = all_ys[:cut], all_ys[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "#TODO\n",
    "# 1. build a network with \n",
    "#  - an input layer\n",
    "#  - some number of dense layers and dropout\n",
    "\n",
    "word_input = Input(shape=(x_train.shape[1],))  # to be removed\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "#output = ...\n",
    "\n",
    "model = Model(word_input, output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, validation_data=(x_val, y_val), shuffle=True, epochs=16, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to process new sentences so that they can be processed by our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"Preprocess a sentence into word vectors for the model\n",
    "    \n",
    "    TODO:\n",
    "        1. split sentence into words (and make lowercase)\n",
    "        2. make each word into a 3-word window (use seq_to_windows)\n",
    "            - this will require the creating some fake tags in the right format\n",
    "                in order to pass to `seq_to_windows`\n",
    "        3. turn each row (3-word window) into a single vector (use window_row_to_vector)\n",
    "        4. turn the list of 150-d vectors into a numpy matrix shape (n_words x 150)\n",
    "    \"\"\"\n",
    "    words = sentence.lower().split()\n",
    "    # your code here\n",
    "    #fake_tags = ... we don't know the tags\n",
    "    # your code here\n",
    "    #return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    preprocess_sentence('This sentence has five words').shape == \n",
    "    (5, EMBEDDING_DIM * (1 + 2 * WINDOW))\n",
    "), '`preprocess_sentence` does not work'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eric      :  per\n",
      "Schmidt   :  per\n",
      "quit      :  o\n",
      "after     :  o\n",
      "Netflix   :  org\n",
      "Inc       :  org\n",
      "announced :  o\n",
      "it        :  o\n",
      "would     :  o\n",
      "acquire   :  o\n",
      "Google    :  org\n",
      "Inc       :  org\n",
      "for       :  o\n",
      "17        :  o\n",
      "dollars   :  o\n",
      ".         :  o\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "# 1. Come up with a sentence\n",
    "# 2. preprocess it for consumption by the network with `preprocess_sentence`\n",
    "# 3. use the model to make predictions\n",
    "# 4. Turn the predicted probabilities into predicted labels\n",
    "# 5. print the output nicely  (done for you)\n",
    "\n",
    "\n",
    "# Make up some of your own sentences\n",
    "sentence = 'Eric Schmidt quit after Netflix Inc announced it would acquire Google Inc for 17 dollars .'\n",
    "\n",
    "processed_sentence = preprocess_sentence(sentence)\n",
    "#predictions = model.predict...\n",
    "#predicted_labels = ...\n",
    "\n",
    "maxlen = max(map(len, sentence.split()))\n",
    "for word, label in zip(sentence.split(), predicted_labels):\n",
    "    print('{} :  {}'.format(word.ljust(maxlen), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
